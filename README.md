## Summarization Studies on Sparse Attention Models

Standard Attention Mechanism requirews quadratic complexity depending on the input sequence length. This is major drwaback in Text Summarization Task, which takes long sequences as input values. To mend this problem, sparse attention mechanism has introduced, and the major pretrained model with this sparse attention mechanism is **Big Bird** and **Longformer**.
This repo uses Two models in two different strategies. And we'll trying to figure out pros and cons of using sparse attention mechanism.



<br>
<br>

## Models

**Longformer** <br>

<br>

**Big Bird** <br>

<br>
<br>


## Strategies

**Simple Fine Tuning** <br>

<br>

**Fused Fine Tuning** <br>

<br>
<br>


## Experimental Setup

<br>
<br>

## Result

<br>
<br>

## How to Use

<br>
<br>

## Reference

<br>
