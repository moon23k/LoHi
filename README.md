## LoHi

The core idea in Transformer is a Self Attention Mechanism, which hinders handling long sequence.
Many studies have been conducted to solve this problem. There are two representative approaches. One is to adopt a hierarchical Encoder structure and the other is to improve the Attention Mechanism.
In this repo, two models for each methodology, a total of four models are compared to find suitable structure for handling long sentences



<br>
<br>

## Models
There are a total of four models, two for each of the two methodologies **Hierarchical** and **Advanced Attention**.
For a fair comparison, only the Encoder is different and the Decoder is identical to that of the Vanilla Transformer.

<br>

> ### Hierarchical Structures

&nbsp; &nbsp; **Hierarchical Recurrent Transformer**

<br>

&nbsp; &nbsp; **Hierarchical Transformer**

<br>
<br>

> ### Attention Improved Structures

&nbsp; &nbsp; **Reformer**

<br>

&nbsp; &nbsp; **BigBird**


<br>
<br>

## Configs

**Model Configs**

<br>

**Training Configs**

<br>
<br>

## Results

<br>
<br>

## How to Use

<br>
<br>

## References
**Attention is all you need**

**Reformer**

**BigBird**
