## Summarization Pooling Or Sparse?

Most current NLP models, including BERT and GPT, are based on Transformers Architecture. The core idea of Transformer is attention mechanism, which has a quadratic complexity, so there is a limit to dealing with long sentences. To solve this, **Pooling** the representation of each sentence through a hierarchical encoder or **Sparse Attention** are usually utilized. This repo compares two methods in detail.



<br>
<br>

## Strategies

**Pooling** <br>

**Sparse Attention**

<br>
<br>

## Experimental Setup

<br>
<br>

## Result

<br>
<br>

## How to Use

<br>
<br>

## Reference

<br>
